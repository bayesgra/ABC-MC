{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f2d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from enum import Enum\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pprint\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from scipy.stats import levy_stable\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45b19e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance labels\n",
    "class Distance(Enum):\n",
    "    CVM = 0\n",
    "    MMD = 1\n",
    "    WASS = 2\n",
    "    STAT = 3  # New distance: Euclidean distance between means\n",
    "\n",
    "DISTANCE_LABELS = {Distance.CVM: \"CvM\", Distance.MMD: \"MMD\", Distance.WASS: \"Wass\", Distance.STAT: \"Stat\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1150b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute the distances\n",
    "\n",
    "# Common to all examples\n",
    "def cramer_von_mises_distance(obs, sim):\n",
    "    n = len(obs)\n",
    "    m = len(sim)\n",
    "    if n == 0 or m == 0:\n",
    "        return np.nan  # or return 0 or a large constant depending on use case\n",
    "\n",
    "    combined = np.concatenate((obs, sim))\n",
    "    ranks = rankdata(combined)\n",
    "    obs_ranks = np.sort(ranks[:n])\n",
    "    sim_ranks = np.sort(ranks[n:])\n",
    "    i = np.arange(1, n + 1)\n",
    "    j = np.arange(1, m + 1)\n",
    "    term1 = n * np.sum((obs_ranks - i) ** 2)\n",
    "    term2 = m * np.sum((sim_ranks - j) ** 2)\n",
    "    denom = n * m * (n + m)\n",
    "    distance = (term1 + term2) / denom - (4 * n * m - 1) / (6 * (n + m))\n",
    "    return distance\n",
    "\n",
    "# Common to all examples\n",
    "def wasserstein_distance(obs, sim):\n",
    "    if len(obs) == len(sim):\n",
    "        sorted_obs = np.sort(obs)\n",
    "        sorted_sim = np.sort(sim)\n",
    "        return np.mean(np.abs(sorted_obs - sorted_sim))\n",
    "    else:\n",
    "        # fallback to scipy implementation if different sizes\n",
    "        from scipy.stats import wasserstein_distance as ws\n",
    "        return ws(obs, sim)\n",
    "\n",
    "# Common to all examples\n",
    "def gaussian_kernel(sq_distances, sigma):\n",
    "    return np.exp(-sq_distances / (2 * sigma))\n",
    "\n",
    "# Common to all examples\n",
    "def maximum_mean_discrepancy(obs, sim, obs_sq_dist=None, sigma=None):\n",
    "    if obs_sq_dist is None:\n",
    "        obs_sq_dist = pdist(obs.reshape(-1,1), 'sqeuclidean')\n",
    "    if sigma is None:\n",
    "        sigma = np.median(obs_sq_dist) ** 0.5\n",
    "    sim_sq_dist = pdist(sim.reshape(-1,1), 'sqeuclidean')\n",
    "    mixed_sq_dist = cdist(obs.reshape(-1,1), sim.reshape(-1,1), 'sqeuclidean')\n",
    "    k_xx = np.mean(gaussian_kernel(obs_sq_dist, sigma))\n",
    "    k_yy = np.mean(gaussian_kernel(sim_sq_dist, sigma))\n",
    "    k_xy = np.mean(gaussian_kernel(mixed_sq_dist, sigma))\n",
    "    return k_xx + k_yy - 2 * k_xy\n",
    "\n",
    "# Specific to the toad example\n",
    "def statistical_distance(x, y):\n",
    "    q = np.linspace(0.0, 1.0, 11)\n",
    "    xq = np.quantile(x, q)\n",
    "    yq = np.quantile(y, q)\n",
    "    with np.errstate(divide='ignore'):\n",
    "        logdiff = np.abs(np.log1p(xq) - np.log1p(yq))\n",
    "    return np.nansum(logdiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462c7e5e-a61d-4af0-9b07-5907cb625341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to simulate for the three toad movement models\n",
    "class Model(Enum):\n",
    "    RANDOM = 0\n",
    "    NEAREST = 1\n",
    "    DISTANCE = 2\n",
    "    \n",
    "def distance_based_probs(position: float, refuge_locations: np.ndarray, p0: float, d0: float) -> np.ndarray:\n",
    "    # Calculating individual return probabilties based on the current position compared to the \n",
    "    # refuge locations for the distance-based return model\n",
    "    refuge_distances = np.abs(position - refuge_locations)\n",
    "    \n",
    "    return p0 * np.exp(-refuge_distances / d0)\n",
    "\n",
    "def toad_movement_sample(model: Model, alpha: float, gamma: float, p0: float, d0: float = None, num_toads: int = 66, num_days: int = 63) -> np.ndarray:\n",
    "    \n",
    "    # Storing 0 as initial refuge for distance-based return\n",
    "    toad_positions = np.zeros((num_days, num_toads))\n",
    "    if model == Model.DISTANCE:\n",
    "        refuge_counts = np.ones(num_toads, dtype=int)\n",
    "        refuge_locations = np.zeros((num_days, num_toads))\n",
    "    else:\n",
    "        no_return_probs = 1 - p0\n",
    "    \n",
    "    # Simulation of step sizes for each toad over the tracking period\n",
    "    steps = levy_stable.rvs(alpha, 0, scale=gamma, size=(num_days - 1, num_toads))\n",
    "    \n",
    "    # Main loop, all toads are handled in one loop to make use of vectorised calculations\n",
    "    for i in range(1, num_days):\n",
    "        # Calculating new position\n",
    "        new_pos = toad_positions[i - 1] + steps[i - 1]\n",
    "        \n",
    "        # Calculating no return probability for distance-based return model (not constant)\n",
    "        if model == Model.DISTANCE:\n",
    "            refuge_probs = [distance_based_probs(new_pos[j], refuge_locations[:refuge_counts[j], j], p0, d0) for j in range(num_toads)]\n",
    "            no_return_probs = np.array([np.prod(1 - refuge_probs[j]) for j in range(num_toads)])\n",
    "        \n",
    "        # Separating toads which are return and not returning for the current day\n",
    "        no_return_flag = np.random.uniform(size=num_toads) < no_return_probs\n",
    "        no_return_ids = np.nonzero(no_return_flag)[0]\n",
    "        return_ids = np.nonzero(~no_return_flag)[0]\n",
    "        \n",
    "        # Updating toad position for non returning toads to th new positions\n",
    "        toad_positions[i, no_return_ids] = new_pos[no_return_ids]\n",
    "        \n",
    "        if model == Model.RANDOM:\n",
    "            # Randomly selecting a location among all previous locations for returning toads\n",
    "            return_location_ids = np.random.randint(0, i, size=return_ids.shape)\n",
    "            toad_positions[i, return_ids] = toad_positions[return_location_ids, return_ids]\n",
    "        elif model == Model.NEAREST:\n",
    "            # Determining nearest return location for each return toad\n",
    "            return_location_ids = np.argmin(np.abs(new_pos[return_ids] - toad_positions[:i, return_ids]), axis=0)\n",
    "            toad_positions[i, return_ids] = toad_positions[return_location_ids, return_ids]\n",
    "        else:\n",
    "            # Randomly selecting previous location using distance-based probabilities for returning toads\n",
    "            # and updating refuge locations and counts for non-return toads\n",
    "            return_location_ids = [np.random.choice(list(range(refuge_counts[j])), p=refuge_probs[j] / np.sum(refuge_probs[j])) for j in return_ids]\n",
    "            toad_positions[i, return_ids] = refuge_locations[return_location_ids, return_ids]\n",
    "            refuge_locations[refuge_counts[no_return_ids], no_return_ids] = new_pos[no_return_ids]\n",
    "            refuge_counts[no_return_ids] += 1\n",
    "            \n",
    "    return toad_positions\n",
    "\n",
    "# Compute the 48 summaries that will represent the observed dataset\n",
    "def compute_displacement_summaries(Y: np.ndarray, lags=[1,2,4,8], threshold=10.0):\n",
    "    summaries = []\n",
    "    n_days, n_toads = Y.shape\n",
    "    for lag in lags:\n",
    "        displacements = np.abs(Y[lag:, :] - Y[:-lag, :]).flatten()\n",
    "        returns = np.sum(displacements < threshold)\n",
    "        non_returns = displacements[displacements >= threshold]\n",
    "        summaries.append({'returns': returns, 'non_returns': non_returns})\n",
    "    return dict(zip(lags, summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b931574e-108f-478e-ac37-d39ae06ca1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances - this function has similarity with the same function for other examples, but, given the different structure of \n",
    "# the input dataset, compute distances for each lag and type of statistics - so it is specific for the toad example\n",
    "def compute_lag_distances(obs_summary, sim_summaries, omega=0.2):\n",
    "    lag_keys = [1, 2, 4, 8]\n",
    "    n_sim = len(sim_summaries)\n",
    "    all_results = []\n",
    "\n",
    "    for sim_idx, sim_summary in enumerate(sim_summaries):\n",
    "        abs_diff_sum = 0\n",
    "        cvm_vals = []\n",
    "        wass_vals = []\n",
    "        mmd_vals = []\n",
    "        stat_vals = []\n",
    "\n",
    "        for lag in lag_keys:\n",
    "            ret_obs = obs_summary[lag]['returns']\n",
    "            ret_sim = sim_summary[lag]['returns']\n",
    "            nonret_obs = obs_summary[lag]['non_returns']\n",
    "            nonret_sim = sim_summary[lag]['non_returns']\n",
    "\n",
    "            abs_diff_sum += abs(ret_obs - ret_sim)\n",
    "\n",
    "            if len(nonret_obs) > 0 and len(nonret_sim) > 0:\n",
    "                cvm_vals.append(cramer_von_mises_distance(nonret_obs, nonret_sim))\n",
    "                wass_vals.append(wasserstein_distance(nonret_obs, nonret_sim))\n",
    "                mmd_vals.append(maximum_mean_discrepancy(nonret_obs, nonret_sim))\n",
    "                stat_vals.append(statistical_distance(nonret_obs, nonret_sim))\n",
    "            else:\n",
    "                cvm_vals.append(np.nan)\n",
    "                wass_vals.append(np.nan)\n",
    "                mmd_vals.append(np.nan)\n",
    "                stat_vals.append(np.nan)\n",
    "\n",
    "        sim_result = {\n",
    "            'CvM': {'return': abs_diff_sum, 'non_return': np.nanmean(cvm_vals)},\n",
    "            'Wass': {'return': abs_diff_sum, 'non_return': np.nanmean(wass_vals)},\n",
    "            'MMD': {'return': abs_diff_sum, 'non_return': np.nanmean(mmd_vals)},\n",
    "            'Stat': {'return': abs_diff_sum, 'non_return': np.nanmean(stat_vals)}\n",
    "        }\n",
    "\n",
    "        all_results.append(sim_result)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f7698c-a92d-47d1-a60f-6c55fa745f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine the 8 distances (2*number of lags) computed with the compute_lag_distances() function\n",
    "def combine_distances(dist_results, omega=0.2):\n",
    "    n_sim = len(dist_results)\n",
    "    metrics = dist_results[0].keys()\n",
    "\n",
    "    combined_per_metric = {}\n",
    "\n",
    "    for metric in metrics:\n",
    "        ret_dists = np.array([d[metric]['return'] for d in dist_results])\n",
    "        nonret_dists = np.array([d[metric]['non_return'] for d in dist_results])\n",
    "\n",
    "        max_ret = ret_dists.max() if ret_dists.max() > 0 else 1e-10\n",
    "        max_nonret = nonret_dists.max() if nonret_dists.max() > 0 else 1e-10\n",
    "\n",
    "        # Weighted normalized distance per simulation\n",
    "        combined = omega * (ret_dists / max_ret) + (1 - omega) * (nonret_dists / max_nonret)\n",
    "        combined_per_metric[metric] = combined\n",
    "\n",
    "    return combined_per_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0497f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the increased computational time to simulate datasets and extract summaries, we write \n",
    "# functions specific for the toad example, making use of parallel simulation.\n",
    "def simulate_one(args):\n",
    "    model = args\n",
    "    if model == Model.DISTANCE:\n",
    "        alpha = np.random.uniform(1, 2.0)\n",
    "        gamma = np.random.uniform(10, 100)\n",
    "        p0 = np.random.uniform(0, 1)\n",
    "        d0 = np.random.uniform(20, 2000)\n",
    "        Y = toad_movement_sample(model, alpha, gamma, p0, d0)\n",
    "        thetas = (alpha, gamma, p0, d0)\n",
    "    else:\n",
    "        alpha = np.random.uniform(1, 2.0)\n",
    "        gamma = np.random.uniform(10, 100)\n",
    "        p0 = np.random.uniform(0, 1)\n",
    "        Y = toad_movement_sample(model, alpha, gamma, p0)\n",
    "        thetas = (alpha, gamma, p0, np.nan)\n",
    "\n",
    "    summary = compute_displacement_summaries(Y)\n",
    "    return summary, thetas, model.value\n",
    "\n",
    "def simulate_datasets_parallel(n_sim: int, processes: int = None):\n",
    "    if processes is None:\n",
    "        processes = cpu_count()\n",
    "\n",
    "    model_list = ([Model.RANDOM] * (n_sim // 3) +\n",
    "                  [Model.NEAREST] * (n_sim // 3) +\n",
    "                  [Model.DISTANCE] * (n_sim // 3))\n",
    "\n",
    "    with Pool(processes) as pool:\n",
    "        results = pool.map(simulate_one, [m for m in model_list])\n",
    "\n",
    "    summaries, thetas, models = zip(*results)\n",
    "    return np.array(summaries), np.array(thetas), np.array(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99130949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the models and parameters relative to the q% smallest distances - we need a new function \n",
    "# because we are using dictionaries for this example\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_percentile(sim_thetas, sim_models, dists, percentile=1.0):\n",
    "    dists = np.asarray(dists)\n",
    "    sim_thetas = np.asarray(sim_thetas)\n",
    "    sim_models = np.asarray(sim_models)\n",
    "\n",
    "    # Determine threshold\n",
    "    threshold = np.percentile(dists, percentile * 100)\n",
    "    accepted_idx = np.where(dists <= threshold)[0]\n",
    "\n",
    "    # Log how many are accepted\n",
    "    print(f\"Percentile {percentile:.2f}: accepted {len(accepted_idx)} / {len(dists)} simulations\")\n",
    "\n",
    "    if len(accepted_idx) == 0:\n",
    "        num_models = len(np.unique(sim_models))\n",
    "        num_thetas = sim_thetas.shape[1] if sim_thetas.ndim > 1 else 1\n",
    "        return (\n",
    "            np.full(num_models, np.nan),\n",
    "            np.full(num_thetas, np.nan),\n",
    "        )\n",
    "\n",
    "    accepted_models = sim_models[accepted_idx]\n",
    "    accepted_thetas = sim_thetas[accepted_idx]\n",
    "\n",
    "    # Model probabilities\n",
    "    unique_models, counts = np.unique(accepted_models, return_counts=True)\n",
    "    model_probs = np.zeros(len(np.unique(sim_models)))\n",
    "    model_probs[unique_models] = counts / counts.sum()\n",
    "\n",
    "    # Mean theta\n",
    "    theta_means = np.nanmean(accepted_thetas, axis=0)\n",
    "\n",
    "    return model_probs, theta_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24747310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run ABC for one observed dataset\n",
    "def run_abc_for_one_observed(i, obs_data, sim_summaries, sim_thetas, sim_models, percentiles, output_dir):\n",
    "    obs_summary = compute_displacement_summaries(obs_data)\n",
    "    lag_distances = compute_lag_distances(obs_summary, sim_summaries)\n",
    "    combined_all = combine_distances(lag_distances, omega=0.2)\n",
    "\n",
    "    results = {}\n",
    "    for dist_enum in Distance:\n",
    "        dist_name = DISTANCE_LABELS[dist_enum]\n",
    "        combined_dist = combined_all[dist_name]\n",
    "\n",
    "        model_probs_list = []\n",
    "        theta_means_list = []\n",
    "\n",
    "        for perc in percentiles:\n",
    "            model_probs, theta_means = summarize_percentile(sim_thetas, sim_models, combined_dist, perc)\n",
    "\n",
    "            if model_probs is not None and theta_means is not None:\n",
    "                model_probs_list.append(np.array(model_probs[:3]))\n",
    "                theta_means_list.append(np.array(theta_means[:3]))\n",
    "            else:\n",
    "                model_probs_list.append(np.full(3, np.nan))\n",
    "                theta_means_list.append(np.full(3, np.nan))\n",
    "\n",
    "        results[dist_name] = {\n",
    "            'model_probs': model_probs_list,  # shape: (len(percentiles), 3)\n",
    "            'theta_means': theta_means_list   \n",
    "        }\n",
    "\n",
    "    out_path = Path(output_dir) / f\"toad_result_random_{i+1}.npz\"\n",
    "    np.savez(\n",
    "        out_path,\n",
    "        result=results,\n",
    "        index=i,\n",
    "        percentiles=percentiles\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c150d191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simulations...\n",
      "Simulating observed datasets...\n",
      "Running ABC in parallel...\n",
      "Percentile 0.01: accepted 12 / 999 simulations\n",
      "Percentile 0.01: accepted 5 / 999 simulations\n",
      "Percentile 0.00: accepted 1 / 999 simulations\n",
      "Percentile 0.01: accepted 14 / 999 simulations\n",
      "Percentile 0.01: accepted 6 / 999 simulations\n",
      "Percentile 0.00: accepted 2 / 999 simulations\n",
      "Percentile 0.01: accepted 10 / 999 simulations\n",
      "Percentile 0.01: accepted 10 / 999 simulations\n",
      "Percentile 0.00: accepted 2 / 999 simulations\n",
      "Percentile 0.01: accepted 10 / 999 simulations\n",
      "Percentile 0.01: accepted 6 / 999 simulations\n",
      "Percentile 0.00: accepted 2 / 999 simulations\n",
      "Percentile 0.01: accepted 11 / 999 simulations\n",
      "Percentile 0.01: accepted 6 / 999 simulations\n",
      "Percentile 0.00: accepted 2 / 999 simulations\n",
      "Percentile 0.01: accepted 11 / 999 simulations\n",
      "Percentile 0.01: accepted 6 / 999 simulations\n",
      "Percentile 0.00: accepted 2 / 999 simulations\n",
      "Percentile 0.01: accepted 10 / 999 simulations\n",
      "Percentile 0.01: accepted 10 / 999 simulations\n",
      "Percentile 0.00: accepted 2 / 999 simulations\n",
      "Percentile 0.01: accepted 10 / 999 simulations\n",
      "Percentile 0.01: accepted 5 / 999 simulations\n",
      "Percentile 0.00: accepted 1 / 999 simulations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def run_one(indexed_obs, sim_summaries, sim_thetas, sim_models, percentiles, output_dir):\n",
    "    i, obs = indexed_obs\n",
    "    return run_abc_for_one_observed(i, obs, sim_summaries, sim_thetas, sim_models, percentiles, output_dir)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Parameters\n",
    "    n_observed = 100\n",
    "    n_simulations = 100000\n",
    "    percentiles = [0.01, 0.005, 0.001]\n",
    "    output_dir = Path(\"toad/random/\") # change to your folder\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Step 1: Simulate training data\n",
    "    print(\"Generating simulations...\")\n",
    "    sims, thetas, models = simulate_datasets_parallel(n_sim=n_simulations)\n",
    "    sim_summaries = sims\n",
    "\n",
    "    # Step 2: Simulate observed datasets\n",
    "    print(\"Simulating observed datasets...\")\n",
    "    observed_raw = [toad_movement_sample(Model.RANDOM, alpha=1.7, gamma=34, p0=0.6) for _ in range(n_observed)] # Simulation for random return model (Model M1)\n",
    "\n",
    "    # Step 3: Run ABC\n",
    "    print(\"Running ABC in parallel...\")\n",
    "    func = partial(\n",
    "        run_one,\n",
    "        sim_summaries=sim_summaries,\n",
    "        sim_thetas=thetas,\n",
    "        sim_models=models,\n",
    "        percentiles=percentiles,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        indexed_obs = list(enumerate(observed_raw))\n",
    "        all_results = pool.map(func, indexed_obs)\n",
    "\n",
    "    # Step 4: Aggregate results\n",
    "    distance_names = list(DISTANCE_LABELS.values())\n",
    "    n_dist = len(distance_names)\n",
    "    n_perc = len(percentiles)\n",
    "    n_theta = len(thetas[0]) if len(thetas) > 0 else 0\n",
    "\n",
    "    model_probs_summary = np.zeros((n_observed, n_dist, 3, n_perc))\n",
    "    theta_means_summary = np.zeros((n_observed, n_dist, 3, n_perc))\n",
    "\n",
    "    for i, result in enumerate(all_results):\n",
    "        for j, dist_name in enumerate(distance_names):\n",
    "            model_probs = np.array(result[dist_name]['model_probs'])  # shape: (n_perc, 3)\n",
    "            theta_means = np.array(result[dist_name]['theta_means'])  # shape: (n_perc, 3)\n",
    "\n",
    "            # Transpose to (3, n_perc)\n",
    "            model_probs_summary[i, j] = model_probs.T\n",
    "            theta_means_summary[i, j] = theta_means.T\n",
    "\n",
    "    # Step 5: Save results\n",
    "    np.save(output_dir / \"model_probs_summary.npy\", model_probs_summary)\n",
    "    np.save(output_dir / \"theta_means_summary.npy\", theta_means_summary)\n",
    "\n",
    "    return model_probs_summary, theta_means_summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c98c6-4988-46e5-a2bc-23ccb3c241db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
